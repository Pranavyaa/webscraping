import re
import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup
import urllib.parse
import ssl


place=input("enter place you want to get data about -")

values={'q':place}

data=urllib.parse.urlencode(values)
url='https://www.google.com/search?'+data


headers={}
headers['User-Agent']="Mozilla/5.0 (X11; Linux i686)"

request=urllib.request.Request(url,headers=headers)
response=urllib.request.urlopen(request)
response_data=response.read()

fhand= open("sample.txt",'w')
list_of_urls=[]
list_in_file=[]

soup = BeautifulSoup(response_data, 'html.parser')
lst=soup.find_all("a")
#lst stores list of urls along with anchor tag

string_matched=re.compile("http.*")

for url in lst:
    url1=url.get('href')
    lofurl=re.findall(string_matched, url1)

    for i in lofurl:
        list_of_urls.append(i)


#removing of unwanted urls
for u in list_of_urls:
    if(u.find("webcache")!=-1):
        continue
    if (u.find("google") != -1):
        continue
    if (u.find("blogger") != -1):
        continue
    if (u.find("youtube") != -1):
        continue

    position=u.find("&")
    urlscont=u[:position]
    position1= urlscont.find("+")
    if(position1==-1):
        if(u[:position] not in list_in_file):
            list_in_file.append(u[:position])

    else:
        if (urlscont[:position] not in list_in_file):
            list_in_file.append(urlscont[:position1])

#for printing the links
for urllines in list_in_file:
    fhand.write(urllines+"\n")
    print(urllines)



#for writting data into the file from obtained links
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

fhand=open("sample.txt",'r')
fhand1=open("data.txt",'w',encoding='utf8')
lst=[]

for i in fhand:
    lst.append(i)
    print(i)

for l1 in lst:
    url = l1
    fhand1.write("=========================================================================================================")
    fhand1.write("\n")
    fhand1.write(url)
    headers = {}
    headers['User-Agent'] = "Mozilla/5.0 (X11; Linux i686)"

    req = urllib.request.Request(url, headers=headers)
    resp = urllib.request.urlopen(req)
    resp_data = resp.read()

    soup = BeautifulSoup(resp_data, 'html.parser')

    lst1 = soup.find_all('p')
    for data_in_p_tag in lst1:
        if (data_in_p_tag.text == None):
            continue

        #for only printing paragraph(escaping small lines)
        if (len(data_in_p_tag.text)<200):
            continue
        fhand1.write(data_in_p_tag.text)

    fhand1.write("\n")
    fhand1.write("=========================================================================================================")


print("done with printing the data")
